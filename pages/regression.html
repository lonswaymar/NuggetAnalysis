<!DOCTYPE html>
<html>
<head>
    <title>Regression - Nugget Analysis</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        body, html {
            height: 100%;
            font-family: "Georgia", sans-serif;
        }
    </style>
</head>
<body>

<!-- Navbar -->
<div class="w3-top">
    <div class="w3-row w3-padding w3-black">
        <div class="w3-col s2"><a href="../index.html" class="w3-button w3-block w3-black">Introduction</a></div>
        <div class="w3-col s2"><a href="arm.html" class="w3-button w3-block w3-black">ARM</a></div>
        <div class="w3-col s2"><a href="pca.html" class="w3-button w3-block w3-black">PCA</a></div>
        <div class="w3-col s2"><a href="clustering.html" class="w3-button w3-block w3-black">Clustering</a></div>
        <div class="w3-col s2"><a href="regression.html" class="w3-button w3-block w3-black">Regression</a></div>
        <div class="w3-col s2"><a href="naivebayes.html" class="w3-button w3-block w3-black">Naive Bayes</a></div>
        <div class="w3-col s2"><a href="decisiontree.html" class="w3-button w3-block w3-black">Decision Tree</a></div>
        <div class="w3-col s2"><a href="conclusion.html" class="w3-button w3-block w3-black">Conclusion</a></div>
    </div>
</div>

<!-- Page Content -->
<div class="w3-sand w3-grayscale w3-large">
    <div class="w3-container" id="regression" style="margin-top:75px;">
>
        <div class="w3-content" style="max-width:1400px">
            <h5 class="w3-center w3-padding-64"><span class="w3-tag w3-wide">Regression Analysis</span></h5>

            <!-- (a) Logistic Regression Overview -->
            <h6><strong>Overview</strong></h6>
            <p>Logistic regression is used for binary classification problems. It models the probability of a binary outcome using a logistic (sigmoid) function. 
            The logit model predicts the likelihood that a given input belongs to a certain class (e.g., win vs. loss).
            </p>
            <p>This makes logistic regression a great model for predicting whether or not a basketball team will win a given game.
            While most models like to look at historical or season data to predict the outcome of a game, I've chosed to look at halftime statistics for every game of the 2023-2024 season.
            I wonder if simply a teams' statistics accumulated halfway through a basketball game can predict whether a team would win the game. 
            No historical data or full game results were included to keep the model focused solely on performance up to halftime.
            </p>
            
            <p>Linear regression, while not used in this analysis, looks to regress data by fitting the dependent variable to the predictor variables with the expression
                <p style="text-align: center;">
                    $$
                    \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
                    $$
                </p>
            </p>
            
            <p>The coefficients used to multiply each predictor variable are calculated by taking the partial derivative of the sum of squared residuals with respect to the given parameter coefficient.
            </p>
            
            <p>More succinctly written, however, the matrix for the best fit parameters can be found by solving
                <p style="text-align: center;">
                    $$
                    \hat{\beta} = (X^TX)^{-1}X^Ty
                    $$
                </p>
            </p>

            <!-- (b) Dataset -->
            <h6 class="w3-margin-top"><strong>Dataset</strong></h6>
            <p>The dataset for this tab was uniquely constructed using two endpoints from the NBA API. The first endpoint, <code>LeagueGameLog</code>, allowed me to get the 
            win/loss labels for every game in the 2023-2024 season. A snippet from this endpoint is shown below.
            </p>
            <table class="w3-table w3-bordered w3-small">
                <thead>
                    <tr>
                        <th>GAME_ID</th>
                        <th>TEAM_NAME</th>
                        <th>WL</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0022300062</td>
                        <td>Warriors</td>
                        <td>L</td>
                    </tr>
                    <tr>
                        <td>0022300062</td>
                        <td>Suns</td>
                        <td>W</td>
                    </tr>
                    <tr>
                        <td>0022300061</td>
                        <td>Nuggets</td>
                        <td>W</td>
                    </tr>
                </tbody>
            </table>
            
            <p>The second endpoint used to construct the data for this tab comes from the <code>PlayByPlay</code> endpoint - the same one used in the ARM tab. A snippet of the data
            from the <code>PlayByPlay</code> endpoint is shown below.
            </p>
            
            <div style="overflow-x: auto; border: 1px solid #ccc; padding: 8px; max-height: 300px;">
                <table class="w3-table w3-bordered w3-small">
                    <thead>
                        <tr>
                            <th>GAME_ID</th><th>EVENTNUM</th><th>EVENTMSGTYPE</th><th>EVENTMSGACTIONTYPE</th><th>PERIOD</th>
                            <th>WCTIMESTRING</th><th>PCTIMESTRING</th><th>HOMEDESCRIPTION</th><th>NEUTRALDESCRIPTION</th>
                            <th>VISITORDESCRIPTION</th><th>SCORE</th><th>SCOREMARGIN</th><th>PERSON1TYPE</th><th>PLAYER1_ID</th>
                            <th>PLAYER1_NAME</th><th>PLAYER1_TEAM_ID</th><th>PLAYER1_TEAM_CITY</th><th>PLAYER1_TEAM_NICKNAME</th>
                            <th>PLAYER1_TEAM_ABBREVIATION</th><th>PERSON2TYPE</th><th>PLAYER2_ID</th><th>PLAYER2_NAME</th>
                            <th>PLAYER2_TEAM_ID</th><th>PLAYER2_TEAM_CITY</th><th>PLAYER2_TEAM_NICKNAME</th>
                            <th>PLAYER2_TEAM_ABBREVIATION</th><th>PERSON3TYPE</th><th>PLAYER3_ID</th><th>PLAYER3_NAME</th>
                            <th>PLAYER3_TEAM_ID</th><th>PLAYER3_TEAM_CITY</th><th>PLAYER3_TEAM_NICKNAME</th>
                            <th>PLAYER3_TEAM_ABBREVIATION</th><th>VIDEO_AVAILABLE_FLAG</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0042300237</td><td>2</td><td>12</td><td>0</td><td>1</td><td>8:07 PM</td><td>12:00</td>
                            <td>None</td><td>Start of 1st Period (8:07 PM EST)</td><td>None</td><td>None</td><td>None</td>
                            <td>None</td><td>0</td><td>0</td><td>None</td><td></td><td></td><td></td>
                            <td>0</td><td>0</td><td></td><td></td><td></td><td></td><td></td>
                            <td>0</td><td>0</td><td></td><td></td><td></td><td></td><td>0</td>
                        </tr>
                        <tr>
                            <td>0042300237</td><td>4</td><td>10</td><td>0</td><td>1</td><td>8:07 PM</td><td>12:00</td>
                            <td>Jump Ball Jokic vs. Gobert: Tip to Edwards</td><td>None</td><td>None</td><td>None</td><td>None</td>
                            <td>4</td><td>203999</td><td>Nikola JokiÄ‡</td><td>1610612743</td><td>Denver</td><td>Nuggets</td><td>DEN</td>
                            <td>5</td><td>203497</td><td>Rudy Gobert</td><td>1610612740</td><td>Minnesota</td><td>Timberwolves</td><td>MIN</td>
                            <td>5</td><td>1630162</td><td>Anthony Edwards</td><td>1610612740</td><td>Minnesota</td><td>Timberwolves</td><td>MIN</td><td>1</td>
                        </tr>
                        <tr>
                            <td>0042300237</td><td>7</td><td>6</td><td>4</td><td>1</td><td>8:07 PM</td><td>11:47</td>
                            <td>None</td><td>None</td><td>Gobert OFF.Foul (P1) (S.Foster)</td><td>None</td><td>None</td>
                            <td>5</td><td>203497</td><td>Rudy Gobert</td><td>1610612740</td><td>Minnesota</td><td>Timberwolves</td><td>MIN</td>
                            <td>4</td><td>1627750</td><td>Jamal Murray</td><td>1610612743</td><td>Denver</td><td>Nuggets</td><td>DEN</td>
                            <td>1</td><td>0</td><td></td><td></td><td></td><td></td><td>1</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <p>To get the final cleaned dataset, I iterated through all of the game ID's present in the first dataset and summed up all of the instances of certain stats in the <code>PlayByPlay</code>
            for each team participating in each game. In doing this, I was able to get the half-time stats and the eventual outcome (win or lose) of each team for every game in the 2023-2024 season. 
            At this point, I also included some feature engineering. Instead of simply tracking the absolute value of <code>FIELD_GOALS_MADE</code>, for a given team, I also calculated the difference
            between in each stat for both teams. If, for example, team A had 15 field goals made and the other team had 12 field goals made, then the feature engineered <code>FIELD_GOALS_MADE_DIFF</code>
            column would report 3 and -3 for team A and B respectively. The final cleaned dataframe is shown below. 
            </p>
            
            <div style="overflow-x: auto; border: 1px solid #ccc; padding: 8px; max-height: 300px;">
                <table class="w3-table w3-bordered w3-small" style="overflow-x:auto;">
                    <thead>
                        <tr>
                            <th>FIELD_GOAL_MADE</th>
                            <th>FIELD_GOAL_MISSED</th>
                            <th>FREE_THROW</th>
                            <th>REBOUND</th>
                            <th>TURNOVER</th>
                            <th>FOUL</th>
                            <th>VIOLATION</th>
                            <th>SUBSTITUTION</th>
                            <th>EJECTION</th>
                            <th>FIELD_GOAL_MADE_DIFF</th>
                            <th>FIELD_GOAL_MISSED_DIFF</th>
                            <th>FREE_THROW_DIFF</th>
                            <th>REBOUND_DIFF</th>
                            <th>TURNOVER_DIFF</th>
                            <th>FOUL_DIFF</th>
                            <th>TEAM_NAME</th>
                            <th>GAME_ID</th>
                            <th>WL</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>18</td><td>40</td><td>6</td><td>24</td><td>4</td><td>15</td><td>0</td><td>14</td><td>0</td>
                            <td>-4</td><td>17</td><td>-9</td><td>-9</td><td>-5</td><td>8</td><td>Warriors</td><td>22300062</td><td>L</td>
                        </tr>
                        <tr>
                            <td>22</td><td>23</td><td>15</td><td>33</td><td>9</td><td>7</td><td>1</td><td>13</td><td>0</td>
                            <td>4</td><td>-17</td><td>9</td><td>9</td><td>5</td><td>-8</td><td>Suns</td><td>22300062</td><td>W</td>
                        </tr>
                        <tr>
                            <td>22</td><td>27</td><td>6</td><td>24</td><td>5</td><td>12</td><td>1</td><td>13</td><td>0</td>
                            <td>-2</td><td>3</td><td>-5</td><td>0</td><td>0</td><td>9</td><td>Lakers</td><td>22300061</td><td>L</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <p>Note that the table above shows the complete cleaned and labeled dataset that forms the basis of all the other models in this tab and the subsequent two. 
            The labels were removed for all of the modeling. And the models were trained with 80% of the data and tested on the remainder. 
            </p>

            <p>To kick off the logistic regression, my first instinct was to be able to visualize the sigmoid function. But because the feature space of the data was so great, 
            I decided to apply principal component analysis to the data and plot the labels against it. A plot of the sigmoid function is shown below. 
            </p>

           

            <!-- (c) PCA & Visualization -->
            <h6 class="w3-margin-top"><strong>Logistic Regression with PCA</strong></h6>
            <p>PCA was used to reduce the dimensionality of the data to a single feature, allowing a clear visualization of the logistic regression decision boundary. The following plot shows the sigmoid curve fitted to the PCA-transformed feature and binary win/loss outcome:</p>

            <div class="w3-center">
                <img src="../images/module3_images/WL_vs_pca.png" class="w3-image" style="width:100%; max-width:1000px;">
            </div>
            
            <p><strong>Note on logistic regression and maximum likelihood estimation</strong>.
            Logistic regression works by utilizing the sigmoid function to transform the data into a pseudo-linear space. In the pseudo-linear space, a line of slope and intercept are 
            fit to the transformed data. For this example, the model function is actually a line with slope and intecept because the independent variable is a 1-D decomposition of the feature
            space. However, logistic regression can be performed with an unlimited amount of features. In this case, there is still one intercept but each feature has its own coefficient per the 
            expression below:
            
                <p style="text-align: center;">
                    $$
                    \hat{y} = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n) = \frac{1}{1 + e^{-(\beta_0 + \sum_{i=1}^n \beta_i x_i)}}
                    $$
                </p>
            
            Finally, an algorithm like gradient descent is used to <strong>maximize the likelihood</strong> of each data point relative to its 
            classification. That is, the slope of the line in the pseudo-linear space (and later transformed by the sigmoid function) is optimized when the y-axis odds of the sigmoid function 
            are maximized for both classifications of data. 
            </p>
            
            <p>While using a 1-D decomposition of the data using principal component analysis was great for visualizing, it's worthwhile to compare its accuracy performance against a logistic
            regression model using the entire feature space. The next section shows the confusion matrices for each of these scenarios.
            </p>
            
            <!-- (d) Confusion Matrices -->
            <h6 class="w3-margin-top"><strong>Confusion Matrices</strong></h6>
            <p>The confusion matrix for both models are shown below. One model was trained using all features directly, while the other used only PCA-reduced data:</p>
            
            <div class="w3-container" style="max-width:1400px; margin:0 auto;">
                <div class="w3-row-padding w3-center">
                    <div class="w3-col m6">
                        <img src="../images/module3_images/confusion_matrix_logistic_regression_all.png" class="w3-image" style="width:100%; max-height:600px;">
                        <p class="w3-small">Logistic Regression - All Features</p>
                    </div>
                    <div class="w3-col m6">
                        <img src="../images/module3_images/confusion_matrix_logistic_regression_pca.png" class="w3-image" style="width:100%; max-height:600px;">
                        <p class="w3-small">Logistic Regression - PCA-Reduced</p>
                    </div>
                </div>
            </div>
            
            <p>The following table presents the accuracy and errors of both models (PCA 1-D decomposition and the whole feature space) using logistic regression. Note That
            each of the two models used the same testing and training split of the original cleaned data. Ultimatley, the whole model produces a slightly higher accuracy. This makes sense:
            principal component analysis never captures the full variance in the original input data as its algorithm only maximizes the variance along one linear combination of the feature 
            space. Thus, the whole model may pick up on sensitivies in the data not represented by the 1-D decomposition. However, in order to verify more confidently which model was more accurate,
            we'd need to simulate the performance of each model over many testing and training splits. 
            </p>
            
            
            <table class="w3-table w3-bordered w3-small w3-centered">
                <thead>
                    <tr class="w3-light-grey">
                        <th>Model</th>
                        <th>Accuracy</th>
                        <th>Error Rate</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Logistic Regression (Full Feature Space)</td>
                        <td>0.70</td>
                        <td>0.30</td>
                    </tr>
                    <tr>
                        <td>Logistic Regression (PCA - 1 Component)</td>
                        <td>0.68</td>
                        <td>0.32</td>
                    </tr>
                </tbody>
            </table>


            
            <!-- (e) Comparison to Naive Bayes -->
            <h6 class="w3-margin-top"><strong>Comparison to Naive Bayes</strong></h6>
            <p>To contextualize the performance of logistic regression, it was compared to a Multinomial Naive Bayes model using count data. Given that multinomial Bayes
            can only process positive integer values, the data had to cleaned and feature engineered further. Firstly, all of the columns represnting differences between
            team scores were removed. To account for some of the information lost from this feature subtraction, I created one more column called <code>FGA</code> that is the sum
            of the field goals missed and field goals made columns. By adding in the total, maybe the algorithm could have captured how well a team was doing relative to their total volume of
            shots. The cleaned dataset for this portion looks like this:
            
            <div style="overflow-x: auto; border: 1px solid #ccc; padding: 8px; max-width: 1000px;">
                <table class="w3-table w3-bordered w3-small w3-centered">
                    <thead>
                        <tr class="w3-light-grey">
                            <th>FIELD_GOAL_MADE</th>
                            <th>FIELD_GOAL_MISSED</th>
                            <th>FREE_THROW</th>
                            <th>REBOUND</th>
                            <th>TURNOVER</th>
                            <th>FOUL</th>
                            <th>VIOLATION</th>
                            <th>SUBSTITUTION</th>
                            <th>EJECTION</th>
                            <th>FGA</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>18</td><td>40</td><td>6</td><td>24</td><td>4</td><td>15</td><td>0</td><td>14</td><td>0</td><td>58</td></tr>
                        <tr><td>22</td><td>23</td><td>15</td><td>33</td><td>9</td><td>7</td><td>1</td><td>13</td><td>0</td><td>45</td></tr>
                        <tr><td>22</td><td>27</td><td>6</td><td>24</td><td>5</td><td>12</td><td>1</td><td>13</td><td>0</td><td>49</td></tr>
                    </tbody>
                </table>
            </div>

            <p> Finally, the confusion matrix for the multinomial bayes is shown below. The accuracy was 68%.
            </p>
            
            <div class="w3-center">
                <img src="../images/module3_images/confusion_matrix_multinomialNB.png" class="w3-image" style="width:80%; max-width:800px;">
                <p class="w3-small">Multinomial Naive Bayes</p>
            </div>

            <!-- (f) Conclusions -->
            <h6 class="w3-margin-top"><strong>Conclusions</strong></h6>
            <p>Both models achieved reasonable accuracy, with logistic regression offering strong interpretability through the sigmoid curve and feature coefficients. 
            Multinomial Naive Bayes performed competitively, especially given its simplicity, but remained below the two logistic regression models. 
            Each model confirms that halftime statistics can contribute to an accuracy predictive model of a team winning.
            I suggest that the multinomial naive bayes model worked slightly less well because the DIFF statistics were removed from the training. In removing these features,
            all metrics relative to the given game were omitted from the model.
            </p>

            <!-- Download Section -->
            <strong>Downloads</strong>
            <div class="w3-panel w3-leftbar w3-light-grey w3-margin-top">
                <ul>
                    <li>Cleaned dataset with labels used for logistic regression: <a href="../data/module3_data/cleaned_labeled_data.csv" download>cleaned_labeled_data.csv</a></li>
                    <li>Multinomial NB dataset with FGA column added: <a href="../data/module3_data/multinomial_bayes_data.csv" download>multinomial_bayes_data.csv</a></li>
                    <li>Notebook: <a href="../notebooks/logistic_regression.ipynb" download>logistic_regression.ipynb</a></li>
                </ul>
            </div>

        </div>
    </div>
</div>

<!-- Footer -->
<footer class="w3-center w3-light-grey w3-padding-48 w3-large">
    <p>This analysis is done for the University of Colorado, Boulder, as part of the curriculum for CS 5612</p>
    <p>Template provided <a href="https://www.w3schools.com/w3css/default.asp" target="_blank" class="w3-hover-text-green">w3.css</a></p>
</footer>

</body>
</html>
